{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "flter_for_kb_entities=False #True if you want to use only entities existing in KB from MedMentions\n",
    "filter_for_entity_in_annot=False #True if yu only want to use found entities that are annotated in MedMentions\n",
    "\n",
    "use_own_biobert=False #True if you want to use Biobert as NER\n",
    "use_own_NER=False     #True if you want to use own NER based on MedMentions \n",
    "#if both are false, the whole pretrained scispacy pipeline from medmentions_and_training.ipynb will be used\n",
    "\n",
    "#model paths\n",
    "#ownBio:________________biobert/named-entity-recognition/output/all/checkpoint-XXXXX/\n",
    "#ownNER:________________NERdemo/output/model-best\n",
    "#wandBSweepNER:_________wandBSweep/output/model-best\n",
    "\n",
    "#path to trained EL:____model/name_of_trained_model with medmentions_and_training.ipynb\n",
    "\n",
    "path_to_NER_model=\"./NERdemo/output/model-best\"\n",
    "\n",
    "path_to_NEL_model=\"./models/my_nlp_FinalKB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load medmentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "#need a existing kb to filter for entities. specify path to kb\n",
    "#increases runtime of next cell 6 dramatically\n",
    "\n",
    "if flter_for_kb_entities:\n",
    "    import spacy\n",
    "    from spacy.kb import InMemoryLookupKB\n",
    "    nlp = spacy.load(\"en_core_sci_lg\")\n",
    "    kb = InMemoryLookupKB(vocab=nlp.vocab, entity_vector_length=200)\n",
    "    kb.from_disk(knowledgebase_address)\n",
    "    enti=kb.get_entity_strings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "def read_lines(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            yield line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "filter_cuis=False\n",
    "dataset = []\n",
    "file_path='./MedMentions-master/MedMentions-master/full/data/corpus_pubtator.txt'\n",
    "same_pmid=False\n",
    "abstract=False\n",
    "for line in read_lines(file_path):\n",
    "    if len(line.split(\"|\")) == 3 and same_pmid == False:\n",
    "        pmid, type, text = line.split(\"|\")\n",
    "        if type == 't':\n",
    "            same_pmid=True\n",
    "            abstract = False\n",
    "            mentions={}\n",
    "            entities=[]\n",
    "            cuis=[]\n",
    "    if len(line.split(\"|\")) == 3 and same_pmid == True:\n",
    "        abstract = line.split(\"|\")[2]\n",
    "        text_and_abstract=text + ' ' + abstract\n",
    "        abstract=True\n",
    "    if len(line.split('\\t')) == 6 and same_pmid == True :\n",
    "        pmid, start, end, mention, semId, cui = line.split('\\t')\n",
    "        if flter_for_kb_entities:\n",
    "            if cui not in enti:\n",
    "                continue\n",
    "        offset=(int(start),int(end))\n",
    "        links_dict={cui:1.0}\n",
    "        mentions[offset]=links_dict\n",
    "        entities.append((int(start),int(end),cui))\n",
    "        cuis.append(cui)\n",
    "        abstract=False\n",
    "        \n",
    "    if len(line.split('\\t')) == 1 and same_pmid == True and not abstract:\n",
    "        same_pmid = False\n",
    "        dataset.append((text_and_abstract,{\"links\":mentions,\"entities\":entities},pmid ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "test_dataset = []\n",
    "pmids_train=[]\n",
    "file_path_train='./MedMentions-master/MedMentions-master/full/data/corpus_pubtator_pmids_trng.txt'\n",
    "\n",
    "for line in read_lines(file_path_train):\n",
    "    pmids_train.append(line)\n",
    "\n",
    "pmids_test=[]\n",
    "file_path_test='./MedMentions-master/MedMentions-master/full/data/corpus_pubtator_pmids_test.txt'\n",
    "for line in read_lines(file_path_test):\n",
    "    pmids_test.append(line)\n",
    "\n",
    "for pmid_train in pmids_train:\n",
    "    train_dataset.extend(data for data in dataset if data[2]==pmid_train)\n",
    "\n",
    "for pmid_test in pmids_test:\n",
    "    test_dataset.extend(data for data in dataset if data[2]==pmid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_own_biobert:\n",
    "    from spacy.tokens import Doc\n",
    "    from spacy.language import Language\n",
    "    from spacy.tokens import Span\n",
    "\n",
    "    from safetensors.torch import load_model\n",
    "    from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "    from transformers import pipeline\n",
    "\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\", model_max_length=512)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(path_to_NER_model)\n",
    "\n",
    "    ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\")\n",
    "    # Function to integrate BioBERT model into spaCy pipeline\n",
    "    @Language.component('bio')\n",
    "    def ner_bioBERT(doc):\n",
    "        entities=ner(doc.text)\n",
    "        for ent in entities:\n",
    "            span=doc.char_span(ent[\"start\"], ent[\"end\"], label=ent[\"entity_group\"])\n",
    "            try:\n",
    "                doc.set_ents(entities=[span],default=\"unmodified\")\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        #doc.ents = [Span(doc, start, end, label) for start, end, label in entities]\n",
    "        return doc\n",
    "    nlp_nel = spacy.load(path_to_NEL_model)\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    nlp.add_pipe(\"bio\", last=True)\n",
    "    nlp.add_pipe(\"sentencizer\", name=\"sent\", source=nlp_nel)\n",
    "    nlp.add_pipe(\"entity_linker\", name=\"nel\", source=nlp_nel)\n",
    "elif use_own_NER:\n",
    "    nlp_nel = spacy.load(path_to_NEL_model)\n",
    "    nlp = spacy.load(path_to_NER_model)\n",
    "    nlp.add_pipe(\"sentencizer\", name=\"sent\", source=nlp_nel)\n",
    "    nlp.add_pipe(\"entity_linker\", name=\"nel\", source=nlp_nel)\n",
    "else:\n",
    "    nlp = spacy.load(path_to_NEL_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "detected=[]\n",
    "annot=[]\n",
    "for text, true_annot, _ in test_dataset:\n",
    "  doc = nlp_ner(text)\n",
    "  entitiesListGOLD=set()\n",
    "  for link in true_annot[\"links\"].items():\n",
    "    start=link[0][0]\n",
    "    end=link[0][1]\n",
    "    key, _ = list(link[1].items())[0]\n",
    "    entitiesListGOLD.add((start,end,key))\n",
    "  entitiesList=set()\n",
    "  for ent in doc.ents:\n",
    "    entitiesList.add((ent.start_char,ent.end_char,ent.kb_id_))\n",
    "  detected.append(entitiesList)\n",
    "  annot.append(entitiesListGOLD)\n",
    "\n",
    "if filter_for_entity_in_annot:\n",
    "  cleaned=[]\n",
    "  for i, detect in enumerate(detected):\n",
    "    entitiesListGOLD=set()\n",
    "    for val in detect:\n",
    "      for entr in annot[i]:\n",
    "        if val[0] == entr[0] and val[1] ==entr[1]:\n",
    "          entitiesListGOLD.add(val)\n",
    "        else:\n",
    "          continue\n",
    "    cleaned.append(entitiesListGOLD)\n",
    "  \n",
    "\n",
    "tp=0\n",
    "fn=0\n",
    "fp=0\n",
    "\n",
    "if filter_for_entity_in_annot:\n",
    "  for i, enti in enumerate(cleaned):\n",
    "    tp=tp+len(enti.intersection(annot[i]))\n",
    "    fp=fp+len(enti - annot[i])\n",
    "    fn=fn+len(annot[i] - enti)\n",
    "else:\n",
    "  for i, enti in enumerate(detected):\n",
    "    tp=tp+len(enti.intersection(annot[i]))\n",
    "    fp=fp+len(enti - annot[i])\n",
    "    fn=fn+len(annot[i] - enti)\n",
    "\n",
    "recall=tp/(tp+fn)\n",
    "precision=tp/(tp+fp)\n",
    "f1=2*((precision*recall)/(precision+recall))\n",
    "print(recall)\n",
    "print(precision)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "detected=[]\n",
    "annot=[]\n",
    "for text, true_annot, _ in test_dataset:\n",
    "   # print(true_annot[\"links\"][(0,20)])\n",
    "    doc = nlp_ner(text)  # to make this more efficient, you can use nlp.pipe() just once for all the texts\n",
    "    entitiesListGOLD=set()\n",
    "    for link in true_annot[\"links\"].items():\n",
    "      key, _ = list(link[1].items())[0]\n",
    "      entitiesListGOLD.add(key)\n",
    "    entitiesList=set()\n",
    "    for ent in doc.ents:\n",
    "       entitiesList.add(ent.kb_id_)\n",
    "    detected.append(entitiesList)\n",
    "    annot.append(entitiesListGOLD)\n",
    "\n",
    "    \n",
    "if filter_for_entity_in_annot:\n",
    "  cleaned=[]\n",
    "  for i, detect in enumerate(detected):\n",
    "    entitiesListGOLD=set()\n",
    "    for val in detect:\n",
    "      for entr in annot[i]:\n",
    "        if val[0] == entr[0] and val[1] ==entr[1]:\n",
    "          entitiesListGOLD.add(val)\n",
    "        else:\n",
    "          continue\n",
    "    cleaned.append(entitiesListGOLD)    \n",
    "    \n",
    "tp=0\n",
    "fn=0\n",
    "fp=0\n",
    "\n",
    "if filter_for_entity_in_annot:\n",
    "  for i, enti in enumerate(cleaned):\n",
    "    tp=tp+len(enti.intersection(annot[i]))\n",
    "    fp=fp+len(enti - annot[i])\n",
    "    fn=fn+len(annot[i] - enti)\n",
    "else:\n",
    "  for i, enti in enumerate(detected):\n",
    "    tp=tp+len(enti.intersection(annot[i]))\n",
    "    fp=fp+len(enti - annot[i])\n",
    "    fn=fn+len(annot[i] - enti)\n",
    "\n",
    "recall=tp/(tp+fn)\n",
    "precision=tp/(tp+fp)\n",
    "f1=2*((precision*recall)/(precision+recall))\n",
    "print(recall)\n",
    "print(precision)\n",
    "print(f1)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
