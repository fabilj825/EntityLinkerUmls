{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Yl0iK2l7GM-I",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl0iK2l7GM-I",
        "outputId": "4111e8a5-0aaa-4bff-ef54-db1968ab2915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "dataframes_address = \"dataframes/\"\n",
        "knowledgebase_address = \"knowledgebase/\"\n",
        "model_address = \"model/\"\n",
        "\n",
        "medmentions_path = \"MedMentions-master/MedMentions-master/full/data/\"\n",
        "\n",
        "#which scispacy basemodel to use\n",
        "USE_SCIBERT = False\n",
        "USE_Large =True\n",
        "USE_BC5CDR=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "be77b8ee-d627-4d9d-a699-eb3561a59ede",
      "metadata": {
        "id": "be77b8ee-d627-4d9d-a699-eb3561a59ede"
      },
      "outputs": [],
      "source": [
        "def read_lines(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            yield line.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8966eee8-8fa0-4ecc-ba08-d85044fdffcd",
      "metadata": {
        "id": "8966eee8-8fa0-4ecc-ba08-d85044fdffcd"
      },
      "outputs": [],
      "source": [
        "dataset = []\n",
        "file_path = medmentions_path + \"corpus_pubtator.txt\"\n",
        "same_pmid=False\n",
        "abstract=False\n",
        "for line in read_lines(file_path):\n",
        "    if len(line.split(\"|\")) == 3 and same_pmid == False:\n",
        "        pmid, type, text = line.split(\"|\")\n",
        "        if type == 't':\n",
        "            same_pmid=True\n",
        "            abstract = False\n",
        "            mentions={}\n",
        "            entities=[]\n",
        "            cuis=[]\n",
        "    if len(line.split(\"|\")) == 3 and same_pmid == True:\n",
        "        abstract = line.split(\"|\")[2]\n",
        "        text_and_abstract=text + ' ' + abstract\n",
        "        abstract=True\n",
        "    if len(line.split('\\t')) == 6 and same_pmid == True :\n",
        "        pmid, start, end, mention, semId, cui = line.split('\\t')\n",
        "        offset=(int(start),int(end))\n",
        "        links_dict={cui:1.0}\n",
        "        mentions[offset]=links_dict\n",
        "        entities.append((int(start),int(end),\"ORG\"))\n",
        "        cuis.append(cui)\n",
        "        abstract=False\n",
        "\n",
        "    if len(line.split('\\t')) == 1 and same_pmid == True and not abstract:\n",
        "        same_pmid = False\n",
        "        dataset.append((text_and_abstract,{\"links\":mentions,\"entities\":entities},pmid ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7a19a6ff-cf69-4f6c-9534-164cfee898f6",
      "metadata": {
        "id": "7a19a6ff-cf69-4f6c-9534-164cfee898f6"
      },
      "outputs": [],
      "source": [
        "train_dataset = []\n",
        "test_dataset = []\n",
        "pmids_train=[]\n",
        "file_path_train = medmentions_path + \"corpus_pubtator_pmids_trng.txt\"\n",
        "for line in read_lines(file_path_train):\n",
        "    pmids_train.append(line)\n",
        "\n",
        "pmids_test=[]\n",
        "file_path_test = medmentions_path + \"corpus_pubtator_pmids_test.txt\"\n",
        "for line in read_lines(file_path_test):\n",
        "    pmids_test.append(line)\n",
        "\n",
        "for pmid_train in pmids_train:\n",
        "    train_dataset.extend(data for data in dataset if data[2]==pmid_train)\n",
        "\n",
        "for pmid_test in pmids_test:\n",
        "    test_dataset.extend(data for data in dataset if data[2]==pmid_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "acc564bb-abc3-44a5-9b92-5e693966c3a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acc564bb-abc3-44a5-9b92-5e693966c3a3",
        "outputId": "f4391d51-a755-4bc2-a5ff-692053a4d1ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy_transformers/layers/hf_shim.py:137: UserWarning: Error loading saved torch state_dict with strict=True, likely due to differences between 'transformers' versions. Attempting to load with strict=False as a fallback...\n",
            "\n",
            "If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current 'transformers' and 'spacy-transformers' versions. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "if USE_SCIBERT == True and not USE_Large and not USE_BC5CDR:\n",
        "  nlp=spacy.load(\"en_core_sci_scibert\")\n",
        "elif USE_Large == True and not USE_SCIBERT and not USE_BC5CDR:\n",
        "  nlp=spacy.load(\"en_core_sci_lg\")\n",
        "elif USE_Large == False and not USE_SCIBERT and USE_BC5CDR:\n",
        "  nlp=spacy.load(\"en_ner_bc5cdr_md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "87e1d3fd-6296-426a-bcac-1a866661affb",
      "metadata": {
        "id": "87e1d3fd-6296-426a-bcac-1a866661affb"
      },
      "outputs": [],
      "source": [
        "from spacy.training import Example\n",
        "\n",
        "TRAIN_EXAMPLES = []\n",
        "if \"sentencizer\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(\"sentencizer\")\n",
        "sentencizer = nlp.get_pipe(\"sentencizer\")\n",
        "for text, annotation,_ in train_dataset:\n",
        "\n",
        "    try:\n",
        "        example = Example.from_dict(nlp.make_doc(text), annotation)\n",
        "        example.reference = sentencizer(example.reference)\n",
        "        TRAIN_EXAMPLES.append(example)\n",
        "    except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "562c1c31-5cb4-48b3-833c-8a39cb4dd55e",
      "metadata": {
        "id": "562c1c31-5cb4-48b3-833c-8a39cb4dd55e"
      },
      "outputs": [],
      "source": [
        "from spacy.ml.models import load_kb\n",
        "\n",
        "entity_linker = nlp.add_pipe(\"entity_linker\", config={\"incl_prior\": False}, last=True)\n",
        "entity_linker.initialize(get_examples=lambda: TRAIN_EXAMPLES, kb_loader=load_kb(knowledgebase_address + '/mykb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5ff02cef-2d84-4aed-a160-8ffe7d369440",
      "metadata": {
        "id": "5ff02cef-2d84-4aed-a160-8ffe7d369440"
      },
      "outputs": [],
      "source": [
        "from thinc.api import decaying\n",
        "from thinc.api import Adam\n",
        "\n",
        "learn_rates = decaying(0.005, 1e-4)\n",
        "optimizer = Adam(learn_rate=learn_rates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2ade2a42-4e62-40de-bb23-411250b08f7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ade2a42-4e62-40de-bb23-411250b08f7f",
        "outputId": "e2ba8918-5ed6-41f1-dcd0-9d2617b2d77d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<thinc.optimizers.Optimizer object at 0x7d59638b8a40>\n",
            "0\n",
            "0 Losses {'entity_linker': 4.110993485277548}\n",
            "0 Losses {'entity_linker': 4.110993485277548}\n"
          ]
        }
      ],
      "source": [
        "from spacy.util import minibatch, compounding\n",
        "import random\n",
        "\n",
        "learning_rates = []\n",
        "losses_list = []\n",
        "\n",
        "ITER = 10\n",
        "\n",
        "with nlp.select_pipes(enable=[\"entity_linker\"]):   # train only the entity_linker\n",
        "    # ITERATIONS NEED TO BE CHANGED BEFORE EXECUTION\n",
        "    for itn in range(ITER):\n",
        "        print(itn)\n",
        "        random.shuffle(TRAIN_EXAMPLES)\n",
        "        batches = minibatch(TRAIN_EXAMPLES, size=compounding(1.0, 32.0, 1.001))  # increasing batch sizes\n",
        "        losses = {}\n",
        "        for batch in batches:\n",
        "            nlp.update(\n",
        "                batch,\n",
        "                drop=0.2,      # prevent overfitting\n",
        "                losses=losses,\n",
        "                sgd=optimizer,\n",
        "            )\n",
        "\n",
        "            learning_rates.append(optimizer.learn_rate)\n",
        "            losses_list.append(losses[\"entity_linker\"])\n",
        "\n",
        "        if itn % 10 == 0:\n",
        "            print(itn, \"Losses\", losses)   # print the training loss\n",
        "print(itn, \"Losses\", losses)\n",
        "\n",
        "nlp.to_disk(model_address + \"my_nlp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d36df442-9be7-44ac-a92c-5faf8ba3ff2e",
      "metadata": {
        "id": "d36df442-9be7-44ac-a92c-5faf8ba3ff2e"
      },
      "outputs": [],
      "source": [
        "nlp_addr = model_address + \"my_nlp\"\n",
        "nlp.to_disk(nlp_addr)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
